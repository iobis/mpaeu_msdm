% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gbif_download.R
\name{occurrence_gbif_db}
\alias{occurrence_gbif_db}
\title{Download partial or full marine data from GBIF (using AWS export)}
\usage{
occurrence_gbif_db(
  full_mode = TRUE,
  export_path,
  scientificname = NULL,
  taxonid = NULL,
  startdate = NULL,
  enddate = NULL,
  format = NULL,
  backend = "duckdb",
  version = NULL,
  bucket = NULL,
  verbose = FALSE
)
}
\arguments{
\item{full_mode}{if \code{TRUE} (default) then the full export of GBIF is downloaded.
That will consume something around 250GB of space. Note that in this case,
\code{scientificname}, \code{taxonid}, \code{startdate} or/and \code{enddate} are ignored.
If \code{FALSE}, then a partial download is executed based on the filters (one of
\code{scientificname}, \code{taxonid}, \code{startdate} or/and \code{enddate} needs to be supplied)}

\item{export_path}{path to a folder where file will be saved}

\item{scientificname}{the scientific name of the species or the taxon key for
the species obtained from GBIF. The second is preferred and you can obtain
the taxon key using the function \code{\link[=get_gbif_keys]{get_gbif_keys()}}. If a scientific name is
supplied, the function will first look in the GBIF taxonomic backbone for a
match and then use the key to download, but you will have no control on the
matching.}

\item{taxonid}{the AphiaID of the species. If supplied, then the function
will first look into WoRMS for the species name and then obtain the taxon
key from GBIF. Ignored if \code{scientificname} is supplied.}

\item{startdate}{the earliest date on which occurrence took place.}

\item{enddate}{the latest date on which the occurrence took place.}

\item{format}{format to download in case of partial download. One of \code{"parquet"} or
\code{"csv"}. If \code{NULL} (default), then \code{"parquet"} is
downloaded.}

\item{backend}{the backend to be used in case of partial download. One of \code{arrow} or \code{duckdb} (the later is working better).}

\item{version}{the version of the full export to use (see: https://registry.opendata.aws/gbif/).
If \code{NULL}, the latest one is used}

\item{bucket}{which bucket to use. If \code{NULL} the default bucket is used (see \code{\link[gbifdb:gbif_remote]{gbifdb::gbif_remote()}})}

\item{verbose}{if \code{TRUE} messages are printed.}
}
\value{
path to saved file (and files saved)
}
\description{
Download partial or full marine data from GBIF (using AWS export)
}
\details{
Downloading the full dataset is much faster if your query is really big (i.e.
the resulting data have many millions of records), but with the cost of a lot of storage
consumed. Of course, you can later process and reduce the file.

When doing partial download, files are saved partitioned by the taxonomic order.

DuckDB is performing better and is the recommended backend in case of not full download.
}
\examples{
\dontrun{
dir <- tempdir()
occurrence_gbif_db("Leptuca thayeri", dir)
}
}
