% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mp_standardize.R
\name{mp_standardize}
\alias{mp_standardize}
\title{Standardize occurrence data for SDMs}
\usage{
mp_standardize(
  species,
  species_list,
  sdm_base,
  species_folder = "data/raw",
  reader = "polars",
  species_outfolder = "data/species",
  geo_out = TRUE,
  geo_method = "isoforest",
  geo_dists_folder = "data/distances",
  env_out = TRUE,
  env_variable = "bathymetry",
  env_method = "mad",
  out_threshold = 0.01,
  remove_land = TRUE,
  approximate_land = TRUE,
  approximate_limit = 1,
  narm_out = TRUE,
  eval_max_impact = 10,
  min_year = 1950,
  exclude_record_type = NULL,
  add_fao = TRUE,
  fao_areas = "data/fao_areas.parquet",
  verbose = TRUE
)
}
\arguments{
\item{species}{the AphiaID for the species}

\item{species_list}{the list of species, with species taxonID/AphiaID and
GBIF keys.}

\item{sdm_base}{a base \code{SpatRaster} in the target resolution. You can
supply one of the environmental layers that you will use (same extent and
resolution planned for the SDM). Supplying this is highly recommended}

\item{species_folder}{the folder where the species data is located (i.e. the
OBIS and GBIF full exports)}

\item{reader}{which package to use for reading the parquet databases. One of
"polars", "duckdb" or "arrow". If you followed the project steps, then "polars" is
recommended because is faster. But if you are facing any problems, just
change for "arrow" or "duckdb" (the last have better memory usage)}

\item{species_outfolder}{folder to output the files}

\item{geo_out}{if \code{TRUE} assess geographical outliers. See
\code{\link[=outqc_geo]{outqc_geo()}}. Note that at least 5 records are necessary for any outlier
removal procedure (either geographical or environmental)}

\item{geo_method}{which method to use for geographical outliers. One of
"iqr", "mad", "isoforest" or "ensemble"}

\item{geo_dists_folder}{the folder with the distances for geographical
outliers. See note on \code{\link[=outqc_geo]{outqc_geo()}}}

\item{env_out}{if \code{TRUE} assess environmental outliers. See
\code{\link[=outqc_env]{outqc_env()}}}

\item{env_variable}{which variable to use for environmental outliers. One of
"bathymetry", "temperature", "salinity" or "shoredistance"}

\item{env_method}{which method to use for environmental outliers. One of
"iqr", "mad", "isoforest" or "ensemble". Note that for both isoforest and
ensemble, \code{env_variable} is ignored and all are used.}

\item{out_threshold}{the limit of points that can be removed (i.e.
percentage). Should be a value between 0 and 1. The default is 0.01, which
limits the maximum number of points that can be removed to 1\% (as we assume
outliers are extremes).}

\item{remove_land}{if \code{TRUE} remove points from land. If \code{sdm_base}
is supplied, will use it as reference. Otherwise, will use the
\code{\link[obistools:check_onland]{obistools::check_onland()}} function}

\item{approximate_land}{if \code{TRUE} and \code{sdm_base} is supplied, the
function will attempt to approximate points on land to the nearest valid
cell, based on approximate limit. This function uses adjacent cells, so it
is not exactly distance based (as distances may vary according to latitude)}

\item{approximate_limit}{either 1 or 2, the number of adjacent cells to any
direction to try when moving the point on land. 1 uses 'queen' movement,
and 2 uses a 5x5 matrix (2 cells for each side/diagonal). See
\code{\link[terra:adjacent]{terra::adjacent()}} for more information}

\item{narm_out}{if \code{TRUE}, then the function will not only remove the
outliers, but also any NA value that there is (cells for which no distance
or environmental information was found)}

\item{eval_max_impact}{the function attempts to separate one dataset for
evaluation. It will measure the spread of points (see details) and the
impact of the dataset removal on the full dataset (in terms of percentage
of points from the total). You can limit the maximum impact allowed when
chosing a dataset}

\item{min_year}{minimum year to filter the data}

\item{exclude_record_type}{which types of observation ("basisofRecord) are
not permited (i.e. should be excluded).
Should be a character vector with valid values for both OBIS and GBIF (see
\code{\link[rgbif:occ_download]{rgbif::occ_download()}} and \code{\link[robis:occurrence]{robis::occurrence()}}). If NULL, it will use
the default values (see details). For ignoring this parameter, set as \code{NA}}

\item{add_fao}{if \code{TRUE}, add information from FishBase and SeaLifeBase of
where the species is native, endemic or introduced, using FAO areas}

\item{fao_areas}{path to parquet file with FAO areas information (see
\code{\link[rfishbase:fb_tbl]{rfishbase::fb_tbl()}})}

\item{verbose}{if \code{TRUE}, print messages}
}
\value{
saved files (see details)
}
\description{
Standardize occurrence data for SDMs
}
\details{
This function aims to standardize (i.e. prepare) the occurrence data
for use on the SDM. Note that all the underlying functions are available
for the user in the package, so you can assembly your own 'standardize'
function if you need something not supported by the parameters available.
The default values are the ones used on the MPA Europe project.

The function will perform the following steps: 1. Open the most recent
files from both OBIS and GBIF (those that are available) 2. It will then
verify if there are duplicated records, using GeoHash (resolution of 6) and
the year of the record. 3. If \code{remove_land} is \code{TRUE}, will
remove points from land. If \code{approximate_land} is \code{TRUE}, it will
try to bring points on land to the closest valid cell. This is limited by
\code{approximate_limit}, so that points that are above this limit are
discarded. 4. If outlier removal methods were set as \code{TRUE}, it will
remove the outliers according to the methods/thresholds 5. The function
will try to separate one dataset for evaluation. It does that by first
converting points to 1 per cell, per dataset. Then, it measure the "spread"
of the points on the geographical space by applying the follow:
\code{
x_cords <- apply(x_cords, 2, function(x) sum((x - mean(x))^2))
sqrt(sum(x_cords)/nrow(x))
}
Being x_cords the coordinates. Thus, the spread of points is measured by
calculating the root mean square deviation of their coordinates from their
respective means. It will then order the datasets from those with higher to
lower spread. It will select the first 50\% with highest spread. From those,
it will select the one with lowest impact on the number of points ( with
impact measured as the percentage from the total number which the dataset
corresponds). To avoid removing a dataset with high impact on the total
number of points, you can limit it through the argument
\code{eval_max_impact}. If a dataset is available to be used as evaluation,
those are marked as "eval_points", while those for fitting are marked as
"fit_points".

The returned parquet file will contain:
\itemize{
\item decimalLongitude
\item decimalLatitude
\item data_type (either fit or evaluation)
\item dataset_sel (datasetID of the dataset used for evaluation, if available)
\item taxonID
\item species (scientific name)
and if \code{add_fao = TRUE}, a column named "fao_confirmed" that contains
1 or 0, with 1 meaning that the point is within the FAO area considered as
native on FishBase or SeaLifeBase

Basis of record default
If nothing is supplied to \code{exclude_record_type}, then the function will
filter to remove occurrence records from the following types:
\itemize{
\item "FOSSIL_SPECIMEN" or "FossilSpecimen"
\item "LIVING_SPECIMEN" or "LivingSpecimen"
}
}
}
\examples{
\dontrun{
base_rast <- rast("data/env/current/thetao_baseline_depthsurf_mean.tif")
mp_standardize(21510, "data/all_splist_20240319.csv", base_rast)
}
}
